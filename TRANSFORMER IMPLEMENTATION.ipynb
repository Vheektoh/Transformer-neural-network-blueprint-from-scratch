{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d106f8",
   "metadata": {},
   "source": [
    "# Implementing the Transformer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fc8bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\personal\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch, string, gc, tqdm\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional, Union, Tuple, List\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6891a",
   "metadata": {},
   "source": [
    "#### Positional embedding code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb3f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbedding(nn.Module):\n",
    "    def __init__(self, h:int, padding_idx:int, n:int = 1000):\n",
    "        super(PosEmbedding, self).__nit__()\n",
    "        self.h = h\n",
    "        self.n = n\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "    def forward(self, x:torch.IntTensor):\n",
    "        assert len(x.shape) == 2, f'input must be 2D, {len(x.shape)} dimensions given'\n",
    "        N, L = x.shape\n",
    "        \n",
    "        output = torch.zeroes(N, L, self.h, device = x.device)\n",
    "        mask = torch.ones(N, L, self.h, device = x.device)\n",
    "        mask[x == self.padding_idx] = 0\n",
    "        \n",
    "        dimensions = [i for i in range (self.h//2)]\n",
    "        \n",
    "        for idx in range(L):\n",
    "            for i in dimensions:\n",
    "                val = x[:, idx] / (self.n**(2 * i / self.h))\n",
    "                output[:, idx, 2*i] = torch.sin(val)\n",
    "                output[:, idx, (2*i) + 1] = torch.cos(val)\n",
    "                \n",
    "        output = output.masked_fill(mask==0, 0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e6676",
   "metadata": {},
   "source": [
    "In summary, the code snippet above takes in input from  the mathematical formula for the positional embedding ie the sin and cos functions\n",
    "where h = vector size of each word(dimension of vector), the padding for the words which would be 0, n = hyperparameter set to 1000 by default.\n",
    "Then we test/check using the assert function to make sure the input for the batch size and sequence length N,L is 2 dimentional.\n",
    "Then we mask the paddings. The for loop is the application of the sin and cos function that is the formula for positional embedding where the x[:, idx] is used for getting the idx index for each sequence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287e69f",
   "metadata": {},
   "source": [
    "#### Scaled Dot product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67875c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self). __init__ ()\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "    \n",
    "    def forward(self, \n",
    "               Q : Optional[torch.FloatTensor],\n",
    "               V : Optional[torch.FloatTensor],\n",
    "               K : Optional[torch.FloatTensor],\n",
    "               padding_mask : Optional[torch.FloatTensor] = None,\n",
    "               attention_mask : Optional[torch.FloatTensor] = None):\n",
    "        \n",
    "        attn_energy = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attn_energy /= np.sqrt(K.shape[-1])\n",
    "        \n",
    "        if torch.is_tensor(padding_mask):\n",
    "            #shape: [N, seq_len]\n",
    "            padding_mask = padding_mask.unsqueeze(dim = 1).unsqueeze(dim = 2)\n",
    "            attn_energy = attn_energy.masked_fill(padding_mask == 0, -torch.inf)\n",
    "            \n",
    "        if torch.is_tensor(attention_mask):\n",
    "            # shape : [N, seq_len, seq_len]\n",
    "            attention_mask = attention_masktion_mask.unsqueeze(dim = 1)\n",
    "            attn_energy = attn_energy.masked_fill(attention_mask == 0, -torch.inf)\n",
    "            \n",
    "        attn_energy = self.softmax(attn_energy)\n",
    "        output = torch.matmul(attn_energy, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484332ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads:int, input_dim:int, dropout:float = 0.1):\n",
    "        super(MultiHeadedAttention, self). __init__()\n",
    "        assert input_dim % n_heads == 0, 'input_dim must be divisible by n_heads'\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = self.input_dim // self.n_heads\n",
    "        \n",
    "        self.Q_fc = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.K_fc = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        self.V_fc = nn.Linear(input_dim, input_dim, bias = False)\n",
    "        \n",
    "        self.attention = DotProductAttention()\n",
    "        self.fc = nn.Linear(input_dim, self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "               Q:Optional[torch.FloatTensor],\n",
    "               K:Optional[torch.FloatTensor],\n",
    "               V:Optional[torch.FloatTensor],\n",
    "               padding_mask:Optional[torch.FloatTensor] = None,\n",
    "               attention_mask:Optional[torch.FloatTensor] = None):\n",
    "        \n",
    "        assert Q.shape[-1] % self.n_heads == 0\n",
    "        assert K.shape[-1] % self.n_heads == 0\n",
    "        assert V.shape[-1] % self.n_heads == 0\n",
    "        \n",
    "        batch_size,  _,  _ = Q.shape\n",
    "        \n",
    "        Q = self.Q_fc(Q)\n",
    "        K = self.K_fc(K)\n",
    "        V = self.V_fc(V)\n",
    "        \n",
    "        Q = Q.reshape(batch_size, Q.shape[1], self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, K.shape[1], self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, V.shape[1], self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = self.attention(Q, K, V, padding_mask, attention_mask)\n",
    "        attn = attn.permute(0, 2, 1, 3)\n",
    "        attn = attn.reshape(batch_size, -1, self.input_dim)\n",
    "        \n",
    "        output = self.fc(attn)\n",
    "        output = self.dropout_layer(dropout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccd29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim:int, n_heads:int, dim_feedforward:int = 2048, dropout:float = 0.1):\n",
    "        super(TransformerEncoderLayer, self). __init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "    \n",
    "        self.self_attention = MultiHeadedAttention(self.n_heads, self.input_dim, self.dropout)\n",
    "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
    "    \n",
    "        self.pointwise_ffn = nn.Sequential(\n",
    "        nn.Linear(self.input_dim, self.dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.dim_feedforward, self.input_dim)\n",
    "        )\n",
    "    \n",
    "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(self.Dropout)\n",
    "        \n",
    "    def forward(self, x:torch.FloatTensor, src_padding_mask:Optional[torch.FloatTensor] = None):\n",
    "        attn = self.self_attention(x, x, x, src_padding_mask)\n",
    "        x = self.norm1(x + attn)\n",
    "        \n",
    "        output = self.pointwise_ffn(x)\n",
    "        output += x\n",
    "        output = self.norm2(output)\n",
    "        output = self.dropout_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7113d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, model_dim:int, n_encoders:int, src_vocab_size:int, padding_idx:Optional[int] = None, n_heads:int = 8,\n",
    "                dim_feedforward:int = 2048, dropout:float = 0.1):\n",
    "        super(TransformerEncoder, self). __init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.n_encoders = n_encoders\n",
    "        self.padding_idx = padding_idx\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(self.src_vocab_size, self.model_dim, self.padding_idx)\n",
    "        self.pos_embedding = PosEmbedding(self.model_dim, self.padding_idx)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.encoder_layer = nn.ModuleList(self.makeEncoderLayers())\n",
    "        \n",
    "    def forward(self, src:torch.IntTensor, src_padding_mask:Optional[torch.FloatTensor] = None):\n",
    "        word_embeddings = self.word_embedding(src)\n",
    "        pos_embeddings = self.pos_embedding(src)\n",
    "        \n",
    "        output = pos_embeddings + word_embeddings\n",
    "        for layers in self.encoder_layers:\n",
    "            output = layers(output, src_padding_mask)\n",
    "        output = self.dropout_layer(output)\n",
    "        return output\n",
    "    \n",
    "    def makeEncoderLayers(self):\n",
    "        return [\n",
    "            TransformerEncoderLayer(self.model_dim, self.n_heads, self.dim_feedforward, self.dropout) \n",
    "            for i in range (self.n_encoders)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452723fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim:int, n_heads:int, dim_feedforward:int = 2048, dropout:float = 0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.self_attention = MultiHeadedAttention(self.n_heads, self.input_dim, self.dropout)\n",
    "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
    "        self.cross_attention = MultiHeadedAttention(self.n_heads, self.input_dim, self.dropout)\n",
    "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
    "        \n",
    "        self.pointwise_ffn = nn.Sequential(\n",
    "        nn.Linear(self.input_dim, self.dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.dim_feedforward, self.input_dim)\n",
    "        )\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(self.input_dim)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "               x:torch.FloatTensor,\n",
    "               encoder_output:torch.FloatTensor,\n",
    "               src_padding_mask:Optional[torch.FloatTensor] = None,\n",
    "               tgt_padding_mask:Optional[torch.FloatTensor] = None,\n",
    "               attention_mask:Optional[torch.FloatTensor] = None):\n",
    "        \n",
    "        attn1 = self.self_attention(x, x, x, tgt_padding_mask, attention_mask)\n",
    "        x = self.norm1(x + attn1)\n",
    "        \n",
    "        attn2 = self.cross_attention(x, encoder_output, encoder_output, src_padding_mask)\n",
    "        x = self.norm2(x + attn2)\n",
    "        \n",
    "        output = self.pointwise_ffn(x)\n",
    "        output += x\n",
    "        output = self.norm2(output)\n",
    "        output = self.dropout_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, model_dim:int, n_decoders:int, tgt_vocab_size:int, padding_idx:Optional[int] = None, n_heads:int = 8,\n",
    "                dim_feedforward:int = 2048, dropout:float = 0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.n_decoders = n_decoders\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(self.tgt_vocab_size, self.model_dim, self.padding_idx)\n",
    "        self.pos_embedding = PosEmbedding(self.model_dim, self.padding_idx)\n",
    "        self.decoder_layers = nn.ModuleList(self.makeDecoderLayers())\n",
    "        \n",
    "    def forward(self, \n",
    "                tgt:torch.IntTensor, \n",
    "                encoder_output:torch.FloatTensor, \n",
    "                src_padding_mask:Optional[torch.FloatTensor] = None,\n",
    "                tgt_padding_mask:Optional[torch.FloatTensor] = None, \n",
    "                attention_mask:Optional[torch.FloatTensor] = None):\n",
    "        \n",
    "        word_embeddings = self.word_embedding(tgt)\n",
    "        pos_embeddings = self.pos_embedding(tgt)\n",
    "        \n",
    "        output = word_embeddings + pos_embeddings\n",
    "        for layers in self.decoder_layers:\n",
    "            output = layers(output, encoder_output, src_padding_mask, tgt_padding_mask, attention_mask)\n",
    "        return output\n",
    "    \n",
    "    def makeDecoderLayers(self):\n",
    "        return [\n",
    "            TransformerDecoderLayer(self.model_dim, self.n_heads, self.dim_feedforward, self.dropout)\n",
    "            for i in range(self.n_decoders)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e69a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqTransformers(nn.Module):\n",
    "    def __init__(self,\n",
    "                model_dim:int, n_encoders:int,\n",
    "                n_decoders:int, src_vocab_size:int,\n",
    "                tgt_vocab_size:int, src_padding_idx:Optional[int] = None,\n",
    "                tgt_padding_idx:Optional[int] = None, n_heads:int = 8,\n",
    "                dim_feedforward:int = 2048, dropout:float = 0.1, device:str = 'cpu'):\n",
    "        super(Seq2seqTransformers, self).__init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.n_encoders = n_encoders\n",
    "        self.n_decoders = n_decoders\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.src_padding_idx = src_padding_idx\n",
    "        self.tgt_padding_idx = tgt_padding_idx\n",
    "        self.n_heads = n_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = TransformerEncoder(self.model_dim, self.n_encoders, self.src_vocab_size, self.src_padding_idx,\n",
    "                                         self.n_heads, self.dim_feedforward, self.dropout)\n",
    "        \n",
    "        self.decoder = TransformerDecoder(self.model_dim, self.n_decoders, self.src_vocab_size, self.src_padding_idx,\n",
    "                                         self.n_heads, self.dim_feedforward, self.dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(self.model_dim, self.tgt_vocab_size)\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, src:torch.IntTensor, tgt:torch.IntTensor):\n",
    "        batch_size, src_sequence_length = src.shape, tgt_sequence_length = tgt.shape\n",
    "        \n",
    "        #src and padding masks\n",
    "        src_padding_mask = self.paddingMask(x=src, padding_idx = self.src_padding_idx)\n",
    "        tgt_padding_mask = self.paddingMask(x=tgt, padding_idx = self.tgt_padding_idx)\n",
    "        \n",
    "        #tgt attention mask\n",
    "        attention_mask = self.diagonalMask(batch_size, tgt_sequence_length, tgt_sequence_length)\n",
    "        enc_output = self.encoder(src, src_padding_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_padding_mask, tgt_padding_mask, attention_mask)\n",
    "        \n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output\n",
    "    \n",
    "    def paddingMask(self, x:torch.IntTensor, padding_idx:int):\n",
    "        padding_mask = torch.zeroes(*x.shape, device = self.device)\n",
    "        padding_mask[x != padding_idx] = 1\n",
    "        return padding_mask\n",
    "    \n",
    "    def diagonalMask(self, *shape:int):\n",
    "        diagonal_mask = torch.ones(*shape, device = self.device)\n",
    "        diagonal_mask = torch.tril(diagonal_mask)\n",
    "        return diagonal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7541609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
